{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-29T13:34:24.034577Z",
     "start_time": "2023-06-29T13:34:24.031381Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# def load_data(file_path):\n",
    "#     df = pd.read_csv(file_path)\n",
    "#     # 对标签进行编码\n",
    "#     df['label'] = df['label'].map({'positive': 1, 'negative': 0})\n",
    "#     return df\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    # 对标签进行编码\n",
    "    df[df.columns[0]] = df[df.columns[0]].map({'positive': 1, 'negative': 0})\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df, w2v_model):\n",
    "    # 使用word2vec模型将文本转换为词向量\n",
    "    X = df.drop(df.columns[0], axis=1).apply(lambda x: [w2v_model.wv[word] for word in x if word in w2v_model.wv],\n",
    "                                             axis=1)\n",
    "\n",
    "    # 使用零填充使每个样本的长度相同\n",
    "    X = pad_sequences(X, padding='post')\n",
    "\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_val, y_val, activation_func):\n",
    "    model = Sequential()\n",
    "    # model.add(Embedding(input_dim=vocabulary_size,\n",
    "    #                 output_dim=embedding_matrix.shape[1],\n",
    "    #                 input_length=max_length,\n",
    "    #                 weights=[embedding_matrix],\n",
    "    #                 trainable=False))\n",
    "    model.add(Dense(128, activation=activation_func, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T15:43:50.035133Z",
     "start_time": "2023-06-29T15:43:50.032268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m val_df \u001B[38;5;241m=\u001B[39m load_data(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata2/val.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# 预处理数据\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw2v_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m y_train \u001B[38;5;241m=\u001B[39m train_df[train_df\u001B[38;5;241m.\u001B[39mcolumns[\u001B[38;5;241m0\u001B[39m]]\n\u001B[1;32m     10\u001B[0m X_val \u001B[38;5;241m=\u001B[39m preprocess_data(val_df, w2v_model)\n",
      "Cell \u001B[0;32mIn[10], line 27\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[0;34m(df, w2v_model)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocess_data\u001B[39m(df, w2v_model):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;66;03m# 使用word2vec模型将文本转换为词向量\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mw2v_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwv\u001B[49m\u001B[43m[\u001B[49m\u001B[43mword\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw2v_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwv\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# 使用零填充使每个样本的长度相同\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     X \u001B[38;5;241m=\u001B[39m pad_sequences(X, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/pandas/core/frame.py:9423\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[1;32m   9412\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[1;32m   9414\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[1;32m   9415\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   9416\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   9421\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[1;32m   9422\u001B[0m )\n\u001B[0;32m-> 9423\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/pandas/core/apply.py:678\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[0;32m--> 678\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/pandas/core/apply.py:798\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    797\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 798\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    800\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[1;32m    801\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/pandas/core/apply.py:814\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    812\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[1;32m    813\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[0;32m--> 814\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[1;32m    816\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[1;32m    817\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[1;32m    818\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[10], line 27\u001B[0m, in \u001B[0;36mpreprocess_data.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocess_data\u001B[39m(df, w2v_model):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;66;03m# 使用word2vec模型将文本转换为词向量\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     X \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mdrop(df\u001B[38;5;241m.\u001B[39mcolumns[\u001B[38;5;241m0\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: [w2v_model\u001B[38;5;241m.\u001B[39mwv[word] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m w2v_model\u001B[38;5;241m.\u001B[39mwv], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# 使用零填充使每个样本的长度相同\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     X \u001B[38;5;241m=\u001B[39m pad_sequences(X, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[10], line 27\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreprocess_data\u001B[39m(df, w2v_model):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;66;03m# 使用word2vec模型将文本转换为词向量\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     X \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mdrop(df\u001B[38;5;241m.\u001B[39mcolumns[\u001B[38;5;241m0\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: [w2v_model\u001B[38;5;241m.\u001B[39mwv[word] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mword\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mw2v_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwv\u001B[49m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# 使用零填充使每个样本的长度相同\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     X \u001B[38;5;241m=\u001B[39m pad_sequences(X, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/gensim/models/keyedvectors.py:649\u001B[0m, in \u001B[0;36mKeyedVectors.__contains__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    648\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__contains__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[0;32m--> 649\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_index_for\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/gensim/models/keyedvectors.py:646\u001B[0m, in \u001B[0;36mKeyedVectors.has_index_for\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhas_index_for\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[1;32m    638\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can this model return a single index for this key?\u001B[39;00m\n\u001B[1;32m    639\u001B[0m \n\u001B[1;32m    640\u001B[0m \u001B[38;5;124;03m    Subclasses that synthesize vectors for out-of-vocabulary words (like\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    644\u001B[0m \n\u001B[1;32m    645\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 646\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/gensim/models/keyedvectors.py:412\u001B[0m, in \u001B[0;36mKeyedVectors.get_index\u001B[0;34m(self, key, default)\u001B[0m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_index\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    408\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the integer index (slot/position) where the given key's vector is stored in the\u001B[39;00m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;124;03m    backing vectors array.\u001B[39;00m\n\u001B[1;32m    410\u001B[0m \n\u001B[1;32m    411\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 412\u001B[0m     val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkey_to_index\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    413\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m val \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    414\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m val\n",
      "\u001B[0;31mTypeError\u001B[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load('w2v.model')\n",
    "\n",
    "# 加载数据\n",
    "train_df = load_data('data2/train.csv')\n",
    "val_df = load_data('data2/val.csv')\n",
    "\n",
    "# 预处理数据\n",
    "X_train = preprocess_data(train_df, w2v_model)\n",
    "y_train = train_df[train_df.columns[0]]\n",
    "X_val = preprocess_data(val_df, w2v_model)\n",
    "y_val = val_df[val_df.columns[0]]\n",
    "\n",
    "# 训练并保存模型\n",
    "activation_funcs = ['relu', 'sigmoid', 'tanh']\n",
    "model_names = ['nn_relu.model', 'nn_sigmoid.model', 'nn_tanh.model']\n",
    "\n",
    "for activation_func, model_name in zip(activation_funcs, model_names):\n",
    "    model = train_model(X_train, y_train, X_val, y_val, activation_func)\n",
    "    model.save(model_name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T15:44:28.843298Z",
     "start_time": "2023-06-29T15:43:51.134965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 第一步：加载所需的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Activation\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T15:45:17.626498Z",
     "start_time": "2023-06-29T15:45:17.624305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# 第二步：加载数据\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name, header=None)\n",
    "    df.columns = ['label'] + list(range(df.shape[1] - 1))\n",
    "    df['text'] = df[df.columns[1:]].apply(\n",
    "        lambda x: ' '.join(x.dropna().astype(str)),\n",
    "        axis=1\n",
    "    )\n",
    "    df = df[['label', 'text']]\n",
    "    df['text'] = df['text'].apply(lambda x: x.split())\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T15:45:18.156939Z",
     "start_time": "2023-06-29T15:45:18.154474Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "           label                                               text\n0       positive  [I, went, through, two, others, before, findin...\n1       positive  [I, was, doubtful, that, anything, could, surv...\n2       positive  [She, watched, the, accompanying, DVD, video, ...\n3       negative  [I, use, the, black, but, it's, too, creamy, t...\n4       positive  [That, would, be, the, only, improvement, I, w...\n...          ...                                                ...\n639995  positive  [It, works, well, but, my, personal, preferenc...\n639996  positive  [The, fact, that, it, has, a, long, handle, ma...\n639997  negative  [The, battles, feel, like, your, running, thro...\n639998  positive  [No, it, didn't, last, forever, but, consideri...\n639999  negative  [The, unit, was, very, difficult, to, install,...\n\n[640000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>positive</td>\n      <td>[I, went, through, two, others, before, findin...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>positive</td>\n      <td>[I, was, doubtful, that, anything, could, surv...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>positive</td>\n      <td>[She, watched, the, accompanying, DVD, video, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>negative</td>\n      <td>[I, use, the, black, but, it's, too, creamy, t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>[That, would, be, the, only, improvement, I, w...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>639995</th>\n      <td>positive</td>\n      <td>[It, works, well, but, my, personal, preferenc...</td>\n    </tr>\n    <tr>\n      <th>639996</th>\n      <td>positive</td>\n      <td>[The, fact, that, it, has, a, long, handle, ma...</td>\n    </tr>\n    <tr>\n      <th>639997</th>\n      <td>negative</td>\n      <td>[The, battles, feel, like, your, running, thro...</td>\n    </tr>\n    <tr>\n      <th>639998</th>\n      <td>positive</td>\n      <td>[No, it, didn't, last, forever, but, consideri...</td>\n    </tr>\n    <tr>\n      <th>639999</th>\n      <td>negative</td>\n      <td>[The, unit, was, very, difficult, to, install,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>640000 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data(\"data2/train.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T15:45:49.593265Z",
     "start_time": "2023-06-29T15:45:19.227081Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from tinycss2 import tokenizer\n",
    "\n",
    "# 第三步：加载word2vec模型并准备嵌入矩阵\n",
    "# def load_word2vec_model(model_name):\n",
    "#     model = Word2Vec.load(model_name)\n",
    "#     word_vectors = model.wv\n",
    "#     vocabulary_size = len(word_vectors.vocab) + 1\n",
    "#     embedding_matrix = np.zeros((vocabulary_size, word_vectors.vector_size))\n",
    "#     for word, i in tokenizer.word_index.items():\n",
    "#         if word in word_vectors:\n",
    "#             embedding_matrix[i] = word_vectors[word]\n",
    "#     return embedding_matrix, vocabulary_size\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def load_word2vec_model(model_name):\n",
    "    model = Word2Vec.load(model_name)\n",
    "    word_vectors = model.wv\n",
    "    vocabulary_size = len(word_vectors.key_to_index) + 1\n",
    "    embedding_matrix = np.zeros((vocabulary_size, word_vectors.vector_size))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in word_vectors.key_to_index:\n",
    "            embedding_matrix[i] = word_vectors.get_vector(word)\n",
    "    return embedding_matrix, vocabulary_size\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T15:48:07.043696Z",
     "start_time": "2023-06-29T15:48:07.040060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# 第四步：定义神经网络模型\n",
    "def define_model(hidden_activation, vocabulary_size, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    # model.add(Embedding(vocabulary_size, embedding_matrix.shape[1],\n",
    "    #                     weights=[embedding_matrix], trainable=False, input_length=max_length))\n",
    "    model.add(Embedding(input_dim=vocabulary_size,\n",
    "                        output_dim=embedding_matrix.shape[1],\n",
    "                        input_length=max_length,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False))\n",
    "\n",
    "    model.add(Dense(64, activation=hidden_activation, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T15:48:08.010109Z",
     "start_time": "2023-06-29T15:48:08.007184Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (64, 2) and (64, 25) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 44\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m activation \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtanh\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m     43\u001B[0m     model \u001B[38;5;241m=\u001B[39m define_model(activation, vocabulary_size, embedding_matrix)\n\u001B[0;32m---> 44\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m     model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnn_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m activation \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.model\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/dq/5xqbtz5j08j9lc5jblz1ww140000gn/T/__autograph_generated_filecsm5rvrg.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/huangjiabao/.local/share/virtualenvs/PythonCoder-NX0uUe41/lib/python3.9/site-packages/keras/src/backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (64, 2) and (64, 25) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# 第五步：加载数据，训练模型并保存\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_data = load_data('data2/train.csv')\n",
    "val_data = load_data('data2/val.csv')\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(train_data['text'])\n",
    "# # max_length = max([len(s.split()) for s in train_data['text']])\n",
    "# max_length = max([len(s) for s in train_data['text']])\n",
    "#\n",
    "# embedding_matrix, vocabulary_size = load_word2vec_model('w2v.model')\n",
    "#\n",
    "# X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_length, padding='post')\n",
    "# X_val = pad_sequences(tokenizer.texts_to_sequences(val_data['text']), maxlen=max_length, padding='post')\n",
    "#\n",
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(train_data['label'])\n",
    "# y_val = le.transform(val_data['label'])\n",
    "\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "max_length = max([len(s) for s in train_data['text']])\n",
    "\n",
    "embedding_matrix, vocabulary_size = load_word2vec_model('w2v.model')\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_length, padding='post')\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(val_data['text']), maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_data['label'])\n",
    "y_val = le.transform(val_data['label'])\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "# Now create and train your model\n",
    "\n",
    "for activation in ['relu', 'sigmoid', 'tanh']:\n",
    "    model = define_model(activation, vocabulary_size, embedding_matrix)\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n",
    "    model.save('nn_' + activation + '.model')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T16:11:07.235924Z",
     "start_time": "2023-06-29T16:10:21.855944Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "#\n",
    "# # Load datasets\n",
    "# train_data = load_data('data2/train.csv')\n",
    "# val_data = load_data('data2/val.csv')\n",
    "#\n",
    "# # Transform labels into one-hot encoded vectors\n",
    "# y_train = to_categorical(train_data['label'].map({'positive': 1, 'negative': 0}))\n",
    "# y_val = to_categorical(val_data['label'].map({'positive': 1, 'negative': 0}))\n",
    "#\n",
    "# # Define a tokenizer and fit it on training data\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(train_data['text'])\n",
    "#\n",
    "# # Get maximum text length for padding\n",
    "# max_length = max([len(s) for s in train_data['text']])\n",
    "#\n",
    "# # Load the Word2Vec model and get the embedding matrix\n",
    "# embedding_matrix, vocabulary_size = load_word2vec_model('w2v.model')\n",
    "#\n",
    "# # Prepare the input data\n",
    "# X_train = pad_sequences(tokenizer.texts_to_sequences(train_data['text']), maxlen=max_length, padding='post')\n",
    "# X_val = pad_sequences(tokenizer.texts_to_sequences(val_data['text']), maxlen=max_length, padding='post')\n",
    "#\n",
    "# # Create and train the models\n",
    "# for activation in ['relu', 'sigmoid', 'tanh']:\n",
    "#     model = define_model(activation, vocabulary_size, embedding_matrix)\n",
    "#     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n",
    "#     model.save('nn_' + activation + '.model')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T16:07:29.275389Z",
     "start_time": "2023-06-29T16:07:29.274154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
